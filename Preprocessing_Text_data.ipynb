{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Dataframe from the pickle file to manipulate the data\n",
    "df = pd.read_pickle(\"Markdown_Documents_bat2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Filenames                                               Text\n",
      "0   05g01_e.doc  # Greenbelt Act, 2005\\n\\n# S.O. 2005, CHAPTER ...\n",
      "12  06c22_e.doc  # Clean Water Act, 2006\\n\\n# S.O. 2006, CHAPTE...\n",
      "63  06c33_e.doc  # Canadian Public Accountability Board Act (On...\n",
      "71  06g16_e.doc  # Metrolinx Act, 2006\\n\\n# S.O. 2006, CHAPTER ...\n",
      "98  06l21_e.doc  # Français\\n\\n# Legislation Act, 2006\\n\\n# S.O...\n"
     ]
    }
   ],
   "source": [
    "# Combining the text of the same file after parsing\n",
    "df['Text'] = df.groupby(['Filenames'])['Text'].transform(lambda x: '\\n'.join(x))\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Original_Documents/00a03_e.doc', 'Original_Documents/00c01_e.doc', 'Original_Documents/00c43_e.doc', 'Original_Documents/00d34_e.doc', 'Original_Documents/00e17_e.doc', 'Original_Documents/00i37_e.doc', 'Original_Documents/00o06_e.doc', 'Original_Documents/00o19_e.doc', 'Original_Documents/00p04_e.doc', 'Original_Documents/00p13_e.doc', 'Original_Documents/00p18_e.doc', 'Original_Documents/00p36_e.doc', 'Original_Documents/00t05_e.doc', 'Original_Documents/00t08_e.doc', 'Original_Documents/00t16_e.doc', 'Original_Documents/01a10_e.doc', 'Original_Documents/01f05_e.doc', 'Original_Documents/01f20_e.doc', 'Original_Documents/01h04_e.doc', 'Original_Documents/01i18_e.doc', 'Original_Documents/01o03_e.doc', 'Original_Documents/01o31_e.doc', 'Original_Documents/01o32_e.doc', 'Original_Documents/01p16_e.doc', 'Original_Documents/01r28_e.doc', 'Original_Documents/02a16_e.doc', 'Original_Documents/02c30_e.doc', 'Original_Documents/02c34_e.doc', 'Original_Documents/02d14_e.doc', 'Original_Documents/02f33_e.doc', 'Original_Documents/02h03_e.doc', 'Original_Documents/02h10_e.doc', 'Original_Documents/02i13_e.doc', 'Original_Documents/02i18_e.doc', 'Original_Documents/02l24_e.doc', 'Original_Documents/02m30_e.doc', 'Original_Documents/02n04_e.doc', 'Original_Documents/02o08e_e.doc', 'Original_Documents/02o08f_e.doc', 'Original_Documents/02p02_e.doc', 'Original_Documents/02p08_e.doc', 'Original_Documents/02r30_e.doc', 'Original_Documents/02s08_e.doc', 'Original_Documents/02s32_e.doc', 'Original_Documents/02t22_e.doc', 'Original_Documents/02t28_e.doc', 'Original_Documents/02t30_e.doc', 'Original_Documents/02u08_e.doc', 'Original_Documents/03a09_e.doc', 'Original_Documents/03k06_e.doc', 'Original_Documents/04a06_e.doc', 'Original_Documents/04c05_e.doc', 'Original_Documents/04g20_e.doc', 'Original_Documents/04p08_e.doc', 'Original_Documents/05a11_e.doc', 'Original_Documents/05h28_e.doc', 'Original_Documents/05m09_e.doc', 'Original_Documents/05p13_e.doc', 'Original_Documents/05p28_e.doc', 'Original_Documents/05p34_e.doc', 'Original_Documents/05s07_e.doc', 'Original_Documents/06a34_e.doc']\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "# print(df[\"Filenames\"].tolist())\n",
    "print([f\"Original_Documents/{file}\" for file in df[\"Filenames\"].tolist()])\n",
    "print(len(df[\"Filenames\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Filenames                                               Text\n",
      "0   05g01_e.doc  # Greenbelt Act, 2005\\n\\n# S.O. 2005, CHAPTER ...\n",
      "12  06c22_e.doc  # Clean Water Act, 2006\\n\\n# S.O. 2006, CHAPTE...\n",
      "63  06c33_e.doc  # Canadian Public Accountability Board Act (On...\n",
      "71  06g16_e.doc  # Metrolinx Act, 2006\\n\\n# S.O. 2006, CHAPTER ...\n",
      "98  06l21_e.doc  # Français\\n\\n# Legislation Act, 2006\\n\\n# S.O...\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "\n",
    "# Ensuring Document is complete\n",
    "with open(\"Markdown_Document(CURR_LAST).md\", \"w\") as f:\n",
    "    f.write(df[\"Text\"].loc[df.index[len(df[\"Filenames\"].tolist())- 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the freshly cleaned dataframe with the original dataframe\n",
    "dfClean = pd.read_pickle(\"Markdown_Documents_CLEAN.pkl\")\n",
    "df = pd.concat([dfClean, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cleaned data to a pickle file\n",
    "df.to_pickle(\"Markdown_Documents_CLEAN.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "# Check how many files have been cleaned\n",
    "print(len(df['Filenames']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Begin Search for Energy Indicators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfClean = pd.read_pickle(\"Markdown_Documents_CLEAN.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mentions Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Filenames                                               Text\n",
      "19   00c43_e.doc  # City of Kawartha Lakes Act, 2000\\n\\n# S.O. 2...\n",
      "104  00t16_e.doc  # Technical Standards and Safety Act, 2000\\n\\n...\n",
      "270  02c30_e.doc  # Consumer Protection Act, 2002\\n\\n# S.O. 2002...\n",
      "401  02h03_e.doc  # Français\\n\\n# Hydro One Inc. Directors and O...\n",
      "12   06c22_e.doc  # Clean Water Act, 2006\\n\\n# S.O. 2006, CHAPTE...\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "dfCleanEnergy = dfClean[dfClean['Text'].str.contains(\"energy\", case=False)]\n",
    "\n",
    "print(dfCleanEnergy.head())\n",
    "print(len(dfCleanEnergy['Filenames']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sussex-Strategy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
